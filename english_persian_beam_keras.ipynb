{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english_persian_beam_seq2seq_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4O1bWcyUioq"
      },
      "source": [
        "# libraries and corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Jd290IKxCT"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "from copy import copy\n",
        "from bisect import bisect_left\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical, plot_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dCFy6vPf22M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441f2878-76b7-47a6-fb9c-6649ced0f859"
      },
      "source": [
        "!wget http://www.manythings.org/anki/pes-eng.zip\n",
        "!unzip pes-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-10 05:51:14--  http://www.manythings.org/anki/pes-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112927 (110K) [application/zip]\n",
            "Saving to: ‘pes-eng.zip’\n",
            "\n",
            "pes-eng.zip         100%[===================>] 110.28K   236KB/s    in 0.5s    \n",
            "\n",
            "2021-04-10 05:51:15 (236 KB/s) - ‘pes-eng.zip’ saved [112927/112927]\n",
            "\n",
            "Archive:  pes-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: pes.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUcYRLZJ-ZS3"
      },
      "source": [
        "file_path = 'pes.txt'\n",
        "text_file = open('pes.txt', 'r', encoding='utf-8')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcddwMsvUdsJ"
      },
      "source": [
        "# text pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPdda1jF_d8T"
      },
      "source": [
        "eng_signs = '?!;()'\n",
        "# eng_signs = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "fa_signs = '،؟!؛()٪'\n",
        "eng_list = []\n",
        "fa_list = []\n",
        "for line in text_file:\n",
        "  # remove useless part of sentences\n",
        "  line = re.sub('CC-BY(.*)','',line)\n",
        "  # change text to lower case\n",
        "  line = line.lower()\n",
        "  # split english and persian sentence\n",
        "  line_split = line.strip().split('\\t')\n",
        "  eng = line_split[0]\n",
        "  fa = line_split[1]\n",
        "  # replace english signs with single point sign\n",
        "  for sign in eng_signs:\n",
        "    eng = eng.replace(sign, '.')\n",
        "    \n",
        "  # replace persian signs with single point sign\n",
        "  for sign in fa_signs:\n",
        "    fa = fa.replace(sign, '.')\n",
        "  \n",
        "  # replace numbers and clock with tag\n",
        "  eng = re.sub('[0-9]+(|:)[0-9]+','<number>',eng)\n",
        "  fa = re.sub('[0-9]+(|:)[0-9]+','<عدد>',fa)\n",
        "  # replace other signs\n",
        "  eng = eng.replace(':', ' ')\n",
        "  fa = fa.replace(':', ' ')\n",
        "  eng = eng.replace(',', '')\n",
        "  eng = eng.replace('\"',' ')\n",
        "  fa = fa.replace('،', ' ')\n",
        "  fa = fa.replace('«', ' ')\n",
        "  fa = fa.replace('»', ' ')\n",
        "  fa = fa.replace('\"', ' ')\n",
        "  # replace half-space\n",
        "  fa = fa.replace('\\u200c',' ')\n",
        "  fa = fa.replace('\\xa0',' ')\n",
        "  # replace duplicate whitespaces\n",
        "  fa = re.sub(' +', ' ', fa)\n",
        "  eng = re.sub(' +', ' ', eng)\n",
        "  \n",
        "  # get eng sentence\n",
        "  eng = eng.strip().split('.')[0]\n",
        "  # get translations from persian\n",
        "  for tr in fa.strip().split('.'):\n",
        "    if tr != '':\n",
        "      eng_list.append(eng)\n",
        "      fa_list.append(tr.strip())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUL4ADFQmh8l"
      },
      "source": [
        "# encoder input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrtOEfAfaogq"
      },
      "source": [
        "# encoder tokenizer (oov_token is for unknown words)\n",
        "enc_tokenizer = Tokenizer(oov_token=1)\n",
        "enc_tokenizer.fit_on_texts(eng_list)\n",
        "\n",
        "# vocab size (+1 is for reserving padding i.e. index zero)\n",
        "enc_vocab_size = len(enc_tokenizer.word_index) + 1\n",
        "eng_seq_list = enc_tokenizer.texts_to_sequences(eng_list)\n",
        "\n",
        "# max encoder sequence size\n",
        "max_enc_len = len(max(eng_seq_list, key=len))\n",
        "\n",
        "# padding sequences\n",
        "padded_eng_seq = pad_sequences(eng_seq_list, maxlen=max_enc_len)\n",
        "\n",
        "# convert list to numpy array\n",
        "encoder_input_data = np.array(padded_eng_seq)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDfCLVsmkX6"
      },
      "source": [
        "# decoder input & output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ODrNIHKlfYz"
      },
      "source": [
        "# decoder tokenizer (oov_token is for unknown words)\n",
        "dec_tokenizer = Tokenizer(oov_token=1)\n",
        "dec_tokenizer.fit_on_texts(fa_list+['sos eos'])\n",
        "\n",
        "# vocab size (+1 is for reserving padding i.e. index zero)\n",
        "dec_vocab_size = len(dec_tokenizer.word_index) + 1\n",
        "\n",
        "# input sequences\n",
        "dec_input_fa_list = ['sos '+seq for seq in fa_list]\n",
        "dec_input_seq_list = dec_tokenizer.texts_to_sequences(dec_input_fa_list)\n",
        "\n",
        "# reversing sentences is not required\n",
        "# for seq in dec_input_seq_list:\n",
        "#   seq.reverse()\n",
        "\n",
        "# output sequences\n",
        "dec_output_fa_list = [seq+' eos' for seq in fa_list]\n",
        "dec_output_seq_list = dec_tokenizer.texts_to_sequences(dec_output_fa_list)\n",
        "\n",
        "# reversing sentences is not required\n",
        "# for seq in dec_output_seq_list:\n",
        "#   seq.reverse()\n",
        "\n",
        "# max decoder sequence size (both input and output are same in length)\n",
        "max_dec_len = len(max(dec_input_seq_list, key=len))\n",
        "\n",
        "# padding sequences\n",
        "padded_dec_input_seq = pad_sequences(dec_input_seq_list, maxlen=max_dec_len, padding='post')\n",
        "padded_dec_output_seq = pad_sequences(dec_output_seq_list, maxlen=max_dec_len, padding='post')\n",
        "\n",
        "# convert list to numpy array\n",
        "decoder_input_data = np.array(padded_dec_input_seq)\n",
        "decoder_output_data = np.array(padded_dec_output_seq)\n",
        "\n",
        "# vocab size (+1 is for reserving padding i.e. index zero)\n",
        "dec_vocab_size = len(dec_tokenizer.word_index) + 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIzLwAuueY_W"
      },
      "source": [
        "# trainable model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUCbXjqT8JhN"
      },
      "source": [
        "# encoder model\n",
        "encoder_input_layer = Input(shape=(max_enc_len,), name='enc_input')\n",
        "encoder_embedding_layer = Embedding(input_dim=enc_vocab_size, \n",
        "                                    output_dim=300, \n",
        "                                    mask_zero=True,\n",
        "                                    name='enc_embedding')(encoder_input_layer)\n",
        "encoder_outputs, state_h, state_c = LSTM(300, return_state=True, name='enc_lstm')(encoder_embedding_layer)\n",
        "# output of lstm layer (states)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# decoder model\n",
        "decoder_input_layer = Input(shape=(max_dec_len,),name='dec_input')\n",
        "decoder_embedding = Embedding(input_dim=dec_vocab_size, output_dim=300, mask_zero=True, name='dec_embedding')\n",
        "decoder_embedding_layer = decoder_embedding(decoder_input_layer)\n",
        "decoder_lstm = LSTM(300, return_sequences=True, return_state=True, name='dec_lstm')\n",
        "# connecting decoder lstm to encoder lstm state gates\n",
        "decoder_lstm_outputs,_,_ = decoder_lstm(decoder_embedding_layer, initial_state=encoder_states)\n",
        "decoder_dense = Dense(dec_vocab_size, activation='softmax',name='dec_output')\n",
        "# for each decoder lstm cell create a dense softmax predicting most probable word out of vocab\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "\n",
        "# model by encoder and decoder input\n",
        "# decoder_outputs consists of network layers of encoder and decoder\n",
        "# (note: we connected both encoder and decoder in decoder lstm layer by encoder lstm states)\n",
        "# model will make a vocab size batch vector to be used with softmax\n",
        "# output shape is (lstm_cells_num or max_dec_len) * vocab_size \n",
        "model = Model([encoder_input_layer, decoder_input_layer], decoder_outputs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3PZCepo5iV"
      },
      "source": [
        "plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al8JG59gecgp"
      },
      "source": [
        "# compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyiFI_EuFiwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abbdb84-4c04-4be0-f7be-50850148b02f"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# provide encoder input and decoder input as X\n",
        "# provide decoder_output_data as y, which is max_dec_len * vocab_size vectors\n",
        "model.fit([encoder_input_data, decoder_input_data], \n",
        "          np.array(to_categorical(decoder_output_data)),\n",
        "          batch_size=32,\n",
        "          epochs=40,\n",
        "          validation_split=0.2)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "66/66 [==============================] - 38s 68ms/step - loss: 2.0674 - val_loss: 2.6808\n",
            "Epoch 2/40\n",
            "66/66 [==============================] - 2s 35ms/step - loss: 1.7712 - val_loss: 2.7215\n",
            "Epoch 3/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.6902 - val_loss: 2.7239\n",
            "Epoch 4/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.6144 - val_loss: 2.7273\n",
            "Epoch 5/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.5692 - val_loss: 2.7313\n",
            "Epoch 6/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.4957 - val_loss: 2.7153\n",
            "Epoch 7/40\n",
            "66/66 [==============================] - 2s 35ms/step - loss: 1.4569 - val_loss: 2.7348\n",
            "Epoch 8/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.3717 - val_loss: 2.7085\n",
            "Epoch 9/40\n",
            "66/66 [==============================] - 2s 35ms/step - loss: 1.3395 - val_loss: 2.7077\n",
            "Epoch 10/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.2595 - val_loss: 2.7172\n",
            "Epoch 11/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.1973 - val_loss: 2.7415\n",
            "Epoch 12/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.1692 - val_loss: 2.7564\n",
            "Epoch 13/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.1019 - val_loss: 2.7583\n",
            "Epoch 14/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.0534 - val_loss: 2.7631\n",
            "Epoch 15/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 1.0132 - val_loss: 2.7844\n",
            "Epoch 16/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.9521 - val_loss: 2.7987\n",
            "Epoch 17/40\n",
            "66/66 [==============================] - 2s 35ms/step - loss: 0.9109 - val_loss: 2.8189\n",
            "Epoch 18/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.8527 - val_loss: 2.8280\n",
            "Epoch 19/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.8153 - val_loss: 2.8721\n",
            "Epoch 20/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.7678 - val_loss: 2.8725\n",
            "Epoch 21/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.7163 - val_loss: 2.9262\n",
            "Epoch 22/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.6885 - val_loss: 2.9212\n",
            "Epoch 23/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.6265 - val_loss: 2.9348\n",
            "Epoch 24/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.5926 - val_loss: 2.9640\n",
            "Epoch 25/40\n",
            "66/66 [==============================] - 2s 35ms/step - loss: 0.5473 - val_loss: 2.9903\n",
            "Epoch 26/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.5002 - val_loss: 3.0167\n",
            "Epoch 27/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.4680 - val_loss: 3.0375\n",
            "Epoch 28/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.4218 - val_loss: 3.0395\n",
            "Epoch 29/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.3917 - val_loss: 3.0388\n",
            "Epoch 30/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.3482 - val_loss: 3.0843\n",
            "Epoch 31/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.3180 - val_loss: 3.1090\n",
            "Epoch 32/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.2862 - val_loss: 3.1195\n",
            "Epoch 33/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.2565 - val_loss: 3.1229\n",
            "Epoch 34/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.2334 - val_loss: 3.1675\n",
            "Epoch 35/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.2021 - val_loss: 3.1726\n",
            "Epoch 36/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.1803 - val_loss: 3.1871\n",
            "Epoch 37/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.1611 - val_loss: 3.2202\n",
            "Epoch 38/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.1433 - val_loss: 3.2164\n",
            "Epoch 39/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.1273 - val_loss: 3.2475\n",
            "Epoch 40/40\n",
            "66/66 [==============================] - 2s 34ms/step - loss: 0.1122 - val_loss: 3.2619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc42c144b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bDwy3C2Onhd"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZcDFgNJsx_c"
      },
      "source": [
        "# model.save('/content/drive/My Drive/models/my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxEvmyj7ef1q"
      },
      "source": [
        "# create inference models (models for prediction purposes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tC9TdEvhykS"
      },
      "source": [
        "# here we separate encoder model and decoder model fpr prediction purposes\n",
        "# why we did not use previous model?\n",
        "# 1 - previous model had two input encoder input and target translation\n",
        "# for prediction purpose we don't have target sentence\n",
        "# 2 - in each step of prediction, we predict only one word of sentence (prediction step by step)\n",
        "# 3 - in order to use last prediction for new prediction\n",
        "# i.e. predict first word and get new lstm states, use that word and states to predict next word and so on\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "  # separate encoder model\n",
        "  # encoder_states consists of encoder model (embedding and lstm layers) and by given input returns states of last timestep\n",
        "  # encoder_states is lstm states output which it's weights are trained\n",
        "  # we give input sentence< it returns states of trained lstm\n",
        "  encoder_model = Model(encoder_input_layer, encoder_states)\n",
        "\n",
        "  # use decoder single input (lstm will have the same weights trained on multi-input form)\n",
        "  # i.e here only one cell is used instead of multiple cells (one word prediction at a time)\n",
        "  decoder_input_single = Input(shape=(1,))\n",
        "  decoder_input_single_x = decoder_embedding(decoder_input_single)\n",
        "\n",
        "  # placeholder fo decoder states (these are inputs and will be fed by values from encoder states)\n",
        "  # actually we define an input for lstm states to assign states came from encoder output to it\n",
        "  dec_state_h = Input(shape=(300,))\n",
        "  dec_state_c = Input(shape=(300,))\n",
        "  # inital states for decoder lstm\n",
        "  dec_states_input = [dec_state_h, dec_state_c]\n",
        "\n",
        "  # decoder model taken from trained decoder (lstm weights are pre-trained)\n",
        "  # here dec_states_input is initial states for decoder lstm\n",
        "  # and state_h and state_c are output of decoder lstm states\n",
        "  # state_h and state_c are used in circular form, i.e. they are used as initial state for next word\n",
        "  dec_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_single_x, initial_state=dec_states_input)\n",
        "\n",
        "  # store states produced by lstm of decoder\n",
        "  dec_states = [state_h, state_c]\n",
        "  \n",
        "  # use traiedn softmax dense layer from decoder part of model\n",
        "  dec_outputs = decoder_dense(dec_lstm_outputs)\n",
        "  \n",
        "  # give single word + states provided from encoder model as inputs of decoder_model\n",
        "  # [decoder_input_single] + dec_states_input is of type list (\"+\" here is for concating lists)\n",
        "  # give softmax output as model output + states provided from decoder lstm to be used for next prediction\n",
        "  decoder_model = Model([decoder_input_single] + dec_states_input,\n",
        "                        [dec_outputs] + dec_states)\n",
        "  \n",
        "  return encoder_model, decoder_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG_jeaXyemw7"
      },
      "source": [
        "# seq2token converter for inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwaSgIdOo9NT"
      },
      "source": [
        "# function to convert input to sequences\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( enc_tokenizer.word_index[ word ] ) \n",
        "    return pad_sequences( [tokens_list] , maxlen=max_enc_len , padding='pre')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQjc1sTGesyk"
      },
      "source": [
        "# english-to-persian translate method definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILNpcumurCWc"
      },
      "source": [
        "## greedy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVJvrjG8rFvd"
      },
      "source": [
        "## beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlXl2LShrPhK"
      },
      "source": [
        "# get encoder and decoder models\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "def decode_translate(seq):\n",
        "  decoded_translation = ''\n",
        "  for x in seq:\n",
        "    decoded_translation += f' {dec_tokenizer.index_word[x]}'\n",
        "  return decoded_translation\n",
        "\n",
        "def beam_translate(sen, b=3):\n",
        "  '''\n",
        "  translates english to persian with beam search\n",
        "  params:\n",
        "    b: width of search (b == 1 is equal to greedy search) \n",
        "  '''\n",
        "  \n",
        "  # initial constants\n",
        "  eos_index = dec_tokenizer.word_index['eos']\n",
        "  \n",
        "  # result holders\n",
        "  final_results = []\n",
        "  final_probs = []\n",
        "\n",
        "  # remember we passed last states to model output so it returns the output of lstm layer\n",
        "  states_values = enc_model.predict( str_to_tokens(sen) )\n",
        "\n",
        "  # create single length seq for target\n",
        "  # this is initial input for decoder which is 'sos'\n",
        "  empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "  empty_target_seq[0, 0] = dec_tokenizer.word_index['sos']\n",
        "\n",
        "  # initial prediction\n",
        "  dec_outputs, h, c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "\n",
        "  # note: below lists are sorted\n",
        "  # top b predictions\n",
        "  seq_indices = np.argsort(dec_outputs[0,-1,:])[-b:][::-1]\n",
        "  # top b probs\n",
        "  seq_probs = np.log(dec_outputs[0,-1,seq_indices])\n",
        "  # total probability of top b prediction sequences\n",
        "  sum_probs = copy(list(seq_probs))\n",
        "  # last states of sequences (for next)\n",
        "  seq_states = [[h,c] for x in range(b)]\n",
        "\n",
        "  # consider each elemnt in lists as a separate list (to keep sequences of predictions)\n",
        "  seq_indices = [[x] for x in seq_indices]\n",
        "  seq_probs = [[x] for x in seq_probs]\n",
        "\n",
        "  # list of alive predictions sequences (not faced \"eos\" token)\n",
        "  alive_seqs = b\n",
        "\n",
        "  while alive_seqs > 0:\n",
        "    # temp lists\n",
        "    temp_seqs = []\n",
        "    temp_probs = []\n",
        "    temp_sum_probs = []\n",
        "    temp_states = []\n",
        "    for i in range(len(seq_indices)):\n",
        "      # current seq of indices and related probs\n",
        "      seq = copy(seq_indices[i])\n",
        "      probs = copy(seq_probs[i])\n",
        "      states = copy(seq_states[i]) # list object [h,c]\n",
        "\n",
        "      # consider i-th sequence in seq_indices list as input of decoder for next prediction\n",
        "      empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "      # consider last word in current sequence\n",
        "      empty_target_seq[ 0 , 0 ] = seq[-1]\n",
        "      # predict\n",
        "      dec_outputs, h, c = dec_model.predict([empty_target_seq] + states)\n",
        "      # top b prediction indices and probs\n",
        "      temp_indeces = np.argsort(dec_outputs[0,-1,:])[-b:][::-1]\n",
        "      \n",
        "      for index in temp_indeces:\n",
        "        # seq\n",
        "        temp_seqs.append(seq+[index])\n",
        "        # prob\n",
        "        new_prob = dec_outputs[0,-1,index]\n",
        "        temp_probs.append(probs+[np.log(new_prob)])\n",
        "        # sum_prob\n",
        "        temp_sum_probs.append(np.sum(temp_probs[-1])/len(temp_probs[-1]))\n",
        "        # temp_states\n",
        "        temp_states.append([h,c])\n",
        "\n",
        "    # get top 3 items\n",
        "    indices = np.argsort(temp_sum_probs)[-b:][::-1]\n",
        "    seq_indices = [temp_seqs[x] for x in indices]\n",
        "    seq_probs = [temp_probs[x] for x in indices]\n",
        "    sum_probs = [temp_sum_probs[x] for x in indices]\n",
        "    seq_states = [temp_states[x] for x in indices]\n",
        "\n",
        "    all_lists = [seq_indices, seq_probs, sum_probs, seq_states]\n",
        "    # update alive values\n",
        "    for i,seq in enumerate(seq_indices):\n",
        "      if seq[-1] == eos_index or len(seq) > max_dec_len:\n",
        "        final_results.append(decode_translate(seq))\n",
        "        final_probs.append(sum_probs[i])\n",
        "        for x in all_lists:\n",
        "          del x[i]\n",
        "        alive_seqs -= 1\n",
        "  \n",
        "  # print translations\n",
        "  print(f'\\n top {b} translations')\n",
        "  sorted_args = np.argsort(final_probs)[::-1]\n",
        "  for i in sorted_args:\n",
        "    print(final_results[i], ' - sum-logs=', round(final_probs[i],3))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e0PlYXkXVBY",
        "outputId": "1ff91204-1a92-4d15-8ca7-0ce0431697d5"
      },
      "source": [
        "beam_translate('i thought i could trust you', b=3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " top 3 translations\n",
            " فکر کردم می توانم بهت اعتماد کنم eos  - sum-logs= -0.173\n",
            " فکر کردم می خواستی بدانی eos  - sum-logs= -0.784\n",
            " فکر کردم می توانم بهت اعتماد کردم eos  - sum-logs= -0.807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUscE0ySX0zD"
      },
      "source": [
        "# old greedy method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfZAxdbspLTn"
      },
      "source": [
        "# get encoder and decoder models\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "def translate(sen):\n",
        "  # remember we passed last states to model output so it returns the output of lstm layer\n",
        "  states_values = enc_model.predict( str_to_tokens(sen) )\n",
        "\n",
        "  # create single length seq for target\n",
        "  # this is initial input for decoder which is 'sos'\n",
        "  empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "  empty_target_seq[0, 0] = dec_tokenizer.word_index['sos']\n",
        "\n",
        "  # stop condition will trigger when eos is printed\n",
        "  stop_condition = False\n",
        "  decoded_translation = ''\n",
        "  while not stop_condition :\n",
        "      # use encoder provided states for initial step of prediction\n",
        "      # get vector output of lstm and it's output states\n",
        "      # dec_outputs is deocder dense layer softmax output which is a vector of vocab_size length\n",
        "      # we used sigle length sequence so output is single vocab_size vector\n",
        "      dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "      # argmax on vector output\n",
        "      sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "      # predicted word\n",
        "      sampled_word = None\n",
        "\n",
        "      # search dictionary for predicted index\n",
        "      # personal note: we don't need a for loop here, modify it later\n",
        "      for word , index in dec_tokenizer.word_index.items() :\n",
        "          if sampled_word_index == index :\n",
        "              # add predicted word to translation\n",
        "              decoded_translation += ' {}'.format( word )\n",
        "              # store predicted word\n",
        "              sampled_word = word\n",
        "      \n",
        "      # if eos printed or max_length of translation printed, trigger stop_condition\n",
        "      if sampled_word == 'eos' or len(decoded_translation.split()) > max_dec_len:\n",
        "          stop_condition = True\n",
        "          \n",
        "      # consider new prediction as input of decoder for next prediction\n",
        "      empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "      empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "      # h,c are states of decoder lstm provided by prediction\n",
        "      # use them as initial states for lstm of next prediction\n",
        "      states_values = [ h , c ] \n",
        "\n",
        "  print( decoded_translation )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1HaBWMKUk5l",
        "outputId": "1460cbe8-5def-476e-a031-53dc16385341"
      },
      "source": [
        "translate('i thought i could trust you')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " فکر کردم می توانم بهت اعتماد کنم eos\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}